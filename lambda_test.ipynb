{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "aws_lambda = boto3.client('lambda')\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='LabRole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'macs123-deployment' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'macs123-deployment'\n",
    "\n",
    "# Check if the bucket exists\n",
    "bucket_exists = False\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    bucket_exists = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "if not bucket_exists:\n",
    "    s3_client.create_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Bucket '{bucket_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_key = 'my_deployment_package.zip'\n",
    "# Upload the file to S3\n",
    "with open('deployment.zip', 'rb') as f:\n",
    "    s3_client.upload_fileobj(f, bucket_name, zip_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "try:\n",
    "    # If function hasn't yet been created, create it\n",
    "    response = lambda_client.create_function(\n",
    "        FunctionName='process_chunk',\n",
    "        Runtime='python3.11',\n",
    "        Role=role['Role']['Arn'],\n",
    "        Handler='lambda_function.lambda_handler',\n",
    "        Code={\n",
    "            'S3Bucket': bucket_name,\n",
    "            'S3Key': zip_file_key\n",
    "        },\n",
    "        Timeout=300\n",
    "    )\n",
    "except lambda_client.exceptions.ResourceConflictException:\n",
    "    # If function already exists, update it based on the S3 object\n",
    "    response = lambda_client.update_function_code(\n",
    "        FunctionName='process_chunk',\n",
    "        S3Bucket=bucket_name,\n",
    "        S3Key=zip_file_key\n",
    "    )\n",
    "lambda_arn = response['FunctionArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"errorMessage\": \"Unable to import module 'lambda_function': Error importing numpy: you should not try to import numpy from\\n        its source directory; please exit the numpy source tree, and relaunch\\n        your python interpreter from there.\",\n",
      "    \"errorType\": \"Runtime.ImportModuleError\",\n",
      "    \"requestId\": \"1a724bfa-112f-41fc-ac62-0f388730e439\",\n",
      "    \"stackTrace\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the Lambda client\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "# Define the payload\n",
    "payload = {\n",
    "  \"chunks\": [\"test1\" , \"test2\"]\n",
    "}\n",
    " \n",
    "\n",
    "# Invoke the Lambda function\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName='process_chunk',\n",
    "    InvocationType='RequestResponse',\n",
    "    Payload=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "response_payload = json.loads(response['Payload'].read())\n",
    "print(json.dumps(response_payload, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'macs123-deployment'\n",
    "# Specify the model identifier\n",
    "model_identifier = \"Minej/bert-base-personality\"\n",
    "# Download the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_identifier)\n",
    "\n",
    "# Save the tokenizer to a local directory\n",
    "local_dir = \"personality_tokenizer\"\n",
    "tokenizer.save_pretrained(local_dir)\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Specify the directory where the model files are stored\n",
    "model_dir = 'personality_model'\n",
    "# Load the model from the downloaded files\n",
    "model = BertForSequenceClassification.from_pretrained(\"Minej/bert-base-personality\")\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "\n",
    "# Process the model outputs as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for file in os.listdir(local_dir):\n",
    "    s3_client.upload_file(os.path.join(local_dir, file), bucket_name, os.path.join(local_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(model_dir):\n",
    "    s3_client.upload_file(os.path.join(model_dir, file), bucket_name, os.path.join(model_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded personality_tokenizer/special_tokens_map.json to /tmp/personality_tokenizer/special_tokens_map.json\n",
      "Downloaded personality_tokenizer/tokenizer.json to /tmp/personality_tokenizer/tokenizer.json\n",
      "Downloaded personality_tokenizer/tokenizer_config.json to /tmp/personality_tokenizer/tokenizer_config.json\n",
      "Downloaded personality_tokenizer/vocab.txt to /tmp/personality_tokenizer/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "bucket_name = 'macs123-deployment'\n",
    "prefix = 'personality_tokenizer/'\n",
    "\n",
    "# Specify the local directory to save the downloaded files\n",
    "local_dir = '/tmp/personality_tokenizer/'\n",
    "\n",
    "# Create the local directory if it doesn't exist\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "bucket_resource = s3_resource.Bucket(bucket_name)\n",
    "objects_under_prefix = bucket_resource.objects.filter(Prefix=prefix)\n",
    "\n",
    "# Iterate over the objects and print their keys\n",
    "for obj in objects_under_prefix:\n",
    "    key = obj.key\n",
    "    filename = os.path.join(local_dir, os.path.basename(key))\n",
    "    s3_client.download_file(bucket_name, key, filename)\n",
    "    print(f\"Downloaded {key} to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "bucket_name = 'macs123-deployment'\n",
    "prefix = 'personality_model/'\n",
    "local_dir = '/tmp/personality_model/'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "bucket_resource = s3_resource.Bucket(bucket_name)\n",
    "objects_under_prefix = bucket_resource.objects.filter(Prefix=prefix)\n",
    "for obj in objects_under_prefix:\n",
    "    key = obj.key\n",
    "    filename = os.path.join(local_dir, os.path.basename(key))\n",
    "    s3_client.download_file(bucket_name, key, filename)\n",
    "    print(f\"Downloaded {key} to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1037, 7099, 3793, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Specify the directory where the tokenizer files are stored\n",
    "tokenizer_dir = '/tmp/personality_tokenizer/'\n",
    "\n",
    "# Load the tokenizer from the downloaded files\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "# Test tokenizer\n",
    "text = \"This is a sample text.\"\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Print tokenized output\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
